{"cells":[{"cell_type":"markdown","metadata":{"id":"rPm4dATWJWvZ"},"source":["#run if using Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17680,"status":"ok","timestamp":1649430718301,"user":{"displayName":"Thong Phan","userId":"05546116130837665241"},"user_tz":-420},"id":"L0170pogJSp9","outputId":"2f72ae51-23cf-4446-e063-b5eddc043a42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1649430727746,"user":{"displayName":"Thong Phan","userId":"05546116130837665241"},"user_tz":-420},"id":"RAl-Jui8Jsu7","outputId":"c906a524-a80d-4f66-bca4-8687d5ffe0a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/fun deep learning/image-enhancement\n"]}],"source":["cd '/content/drive/MyDrive/fun deep learning/image-enhancement'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14346,"status":"ok","timestamp":1649430742405,"user":{"displayName":"Thong Phan","userId":"05546116130837665241"},"user_tz":-420},"id":"J27GyV2dKRMb","outputId":"ca905f7b-cd35-48ec-b422-57c88a585919"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (7.1.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (1.21.5)\n","Collecting opencv_python==4.5.5.64\n","  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n","\u001b[K     |████████████████████████████████| 60.5 MB 1.4 MB/s \n","\u001b[?25hCollecting pillow_heif\n","  Downloading pillow_heif-0.1.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.6 MB)\n","\u001b[K     |████████████████████████████████| 10.6 MB 34.8 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 30)) (1.4.1)\n","Collecting sk_video\n","  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 26.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 52)) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 57)) (0.11.1+cu111)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 60)) (4.63.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (1.4.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (3.0.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 9)) (2.8.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->-r requirements.txt (line 9)) (3.10.0.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r requirements.txt (line 9)) (1.15.0)\n","Requirement already satisfied: cffi>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from pillow_heif->-r requirements.txt (line 27)) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.14.6->pillow_heif->-r requirements.txt (line 27)) (2.21)\n","Installing collected packages: sk-video, pillow-heif, opencv-python\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.1.2.30\n","    Uninstalling opencv-python-4.1.2.30:\n","      Successfully uninstalled opencv-python-4.1.2.30\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed opencv-python-4.5.5.64 pillow-heif-0.1.11 sk-video-1.1.10\n"]}],"source":["! pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1649430742405,"user":{"displayName":"Thong Phan","userId":"05546116130837665241"},"user_tz":-420},"id":"hESRlAoDLsoC","outputId":"de64ea74-440a-4ad9-c090-0f537322b4a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/fun deep learning/image-enhancement/low-light/RetinexDIP\n"]}],"source":["cd '/content/drive/MyDrive/fun deep learning/image-enhancement/low-light/RetinexDIP'"]},{"cell_type":"markdown","metadata":{"id":"TU9zyKFEJnom"},"source":["#Main code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1427,"status":"ok","timestamp":1649430756337,"user":{"displayName":"Thong Phan","userId":"05546116130837665241"},"user_tz":-420},"id":"HLB19oZCI8ww","outputId":"dd36fd7e-d8c8-49a7-f2c1-65e381d9b866"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<module 'img_utils' from '../../img_utils.py'>"]},"metadata":{},"execution_count":5}],"source":["import sys\n","sys.path.append('../../')\n","import general_utils as gu\n","import img_utils as iu\n","\n","import importlib\n","importlib.reload(iu)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4x66Mq8I8w0"},"outputs":[],"source":["from collections import namedtuple\n","from net import *\n","from net.downsampler import *\n","from net.losses import StdLoss, GradientLoss, ExtendedL1Loss, GrayLoss\n","from net.losses import ExclusionLoss, TVLoss\n","from net.noise import get_noise\n","from PIL import Image, ImageFont, ImageDraw\n","from pillow_heif import register_heif_opener\n","register_heif_opener()\n","\n","import numpy as np\n","import math\n","import torch\n","import cv2\n","from torchvision import transforms\n","import time\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","torch.manual_seed(0)\n","\n","def scale_fill(im, target_width, target_height, return_offset=False):\n","    '''\n","    Resize PIL image keeping ratio and adding zero-valued background.\n","    '''\n","    target_ratio = target_height / target_width\n","    im_ratio = im.height / im.width\n","    if target_ratio > im_ratio:\n","        # It must be fixed by width\n","        resize_width = target_width\n","        resize_height = round(resize_width * im_ratio)\n","    else:\n","        # Fixed by height\n","        resize_height = target_height\n","        resize_width = round(resize_height / im_ratio)\n","\n","    image_resize = im.resize((resize_width, resize_height), Image.ANTIALIAS)\n","    background = Image.new('RGBA', (target_width, target_height), 0)\n","    offset = (round((target_width - resize_width) / 2), round((target_height - resize_height) / 2))\n","    background.paste(image_resize, offset)\n","\n","    if return_offset:\n","        return background.convert('RGB'), offset, image_resize.size \n","\n","    return background.convert('RGB')\n","def append_img(im1, im2, new_size, labels=('im1, im2')):\n","    _, _, new_size = scale_fill(im1, new_size, new_size, True)\n","    im1 = im1.resize(new_size)\n","    im2 = im2.resize(new_size)\n","    w, h = im1.size\n","    background = Image.new('RGBA', (w*2, h), 0)\n","    background.paste(im1, (0,0))\n","    background.paste(im2, (w, 0))\n","    im = background.convert('RGB')\n","    ImageDraw.Draw(im).text((0,0), labels[0], (100,100,100))\n","    ImageDraw.Draw(im).text((w,0), labels[1], (100,100,100))\n","    return im\n","\n","def reverse_scale_fill(im, original_size, offset, image_resize_size):\n","    '''\n","    original_size: (W, H)\n","    image_resize_size: (W, H) : size of the original image in the scalefill image \n","    offset: (W_dim, H_dim)\n","    Example: scalefill from 720, 960 to 960, 960 will have offset=120,0 and image_resize_size=720,960\n","    '''\n","    box = offset[0], offset[1], offset[0]+image_resize_size[0], offset[1]+image_resize_size[1]\n","    return im.crop(box).resize(original_size)\n","\n","EnhancementResult = namedtuple(\"EnhancementResult\", ['reflection', 'illumination'])\n","\n","class Enhancement(object):\n","    def __init__(self, image_name, image, target_size=960, plot_during_training=False, show_every=10, num_iter=300):\n","        self.original_image = image\n","        self.size = image.size # (height, width)\n","        self.target_size = target_size\n","        self.image_np = None\n","        self.images_torch = None\n","        self.plot_during_training = plot_during_training\n","        if plot_during_training: \n","            self.output_path = gu.folder('output/')\n","        # self.ratio = ratio\n","        self.psnrs = []\n","        self.show_every = show_every\n","        self.image_name = image_name\n","        self.num_iter = num_iter\n","        self.loss_function = None\n","        # self.ratio_net = None\n","        self.parameters = None\n","        self.learning_rate = 0.01\n","        self.input_depth = 3  # This value could affect the performance. 3 is ok for natural image, if your\n","                            #images are extremely dark, you may consider 8 for the value.\n","        self.data_type = torch.FloatTensor\n","        self.reflection_net_inputs = None\n","        self.illumination_net_inputs = None\n","        self.original_illumination = None\n","        self.original_reflection = None\n","        self.reflection_net = None\n","        self.illumination_net = None\n","        self.total_loss = None\n","        self.reflection_out = None\n","        self.illumination_out = None\n","        self.current_result = None\n","        self.best_result = None\n","        self._init_all()\n","\n","        print('Original image size:', self.size)\n","\n","    def _init_all(self):\n","        self._init_images()\n","        self._init_decomposition()\n","        self._init_nets()\n","        self._init_inputs()\n","        self._init_parameters()\n","        self._init_losses()\n","\n","\n","    def _maxRGB(self):\n","        '''\n","        self.image: pil image, input low-light image\n","        :return: np, initial illumnation\n","        '''\n","        (R, G, B) = self.image.split()\n","        I_0 = np.array(np.maximum(np.maximum(R, G), B))\n","        return I_0\n","\n","    def _init_decomposition(self):\n","        temp = self._maxRGB() # numpy\n","        # get initial illumination map\n","        #IMPORTANT: min clip value is a hyper param\n","        self.original_illumination = np.clip(np.asarray([temp for _ in range(3)]), 2, 255) #range [0,255]\n","        # get initial reflection\n","        self.original_reflection = self.image_np / self.original_illumination #range [0, 255] / [0,255] = [0, 1]\n","\n","        self.original_illumination = np_to_torch(self.original_illumination).type(self.data_type).to(device)\n","        self.original_reflection = np_to_torch(np.asarray(self.original_reflection)).type(self.data_type).to(device)\n","\n","    def _init_images(self):\n","\n","        self.original_image_torch = transforms.PILToTensor()(self.original_image)\n","        self.image, self.scalefill_offset, self.image_resize_size = scale_fill(self.original_image, self.target_size, self.target_size, return_offset=True)\n","        # self.image = transforms.Resize((512, 512))(self.original_image)\n","        self.image_np = pil_to_np(self.image)  # pil image to numpy\n","        self.image_torch = np_to_torch(self.image_np).type(self.data_type).to(device)\n","\n","    def _init_inputs(self):\n","        if self.image_torch is not None:\n","            size = (self.image_torch.shape[2], self.image_torch.shape[3])\n","            # print(size)\n","        input_type = 'noise'\n","        # input_type = 'meshgrid'\n","        self.reflection_net_input = get_noise(self.input_depth,\n","                                                  input_type, size).type(self.data_type).detach().to(device)\n","        self.illumination_net_input = get_noise(self.input_depth,\n","                                             input_type, size).type(self.data_type).detach().to(device)\n","\n","\n","    def _init_parameters(self):\n","        self.parameters = [p for p in self.reflection_net.parameters()] + \\\n","                          [p for p in self.illumination_net.parameters()]\n","        self.optimizer = torch.optim.Adam(self.parameters, lr=self.learning_rate)\n","        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer,\n","                                                              milestones=[300, 600])\n","\n","\n","    def weight_init(self, m):\n","        classname = m.__class__.__name__\n","        if classname.find('Conv') != -1:\n","            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","            m.weight.data.normal_(0.0, 0.5 * math.sqrt(2. / n))\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","        elif classname.find('BatchNorm') != -1:\n","            m.weight.data.fill_(1)\n","            m.bias.data.zero_()\n","        elif classname.find('Linear') != -1:\n","            n = m.weight.size(1)\n","            m.weight.data.normal_(0, 0.01)\n","            m.bias.data = torch.ones(m.bias.data.size())\n","\n","    def _init_nets(self):\n","        pad = 'zero'\n","        self.reflection_net = skip(self.input_depth, 3,\n","               num_channels_down = [8, 16, 32, 64, 128, 256, 512, 1024],\n","               num_channels_up   = [8, 16, 32, 64, 128, 256, 512, 1024],\n","               num_channels_skip = [0, 0, 0, 0, 0, 0, 0, 0],\n","               filter_size_down = 3, filter_size_up = 3, filter_skip_size=1,\n","               upsample_mode='bilinear',\n","               downsample_mode='avg',\n","               need_sigmoid=True, need_bias=True, pad=pad)\n","        self.reflection_net.apply(self.weight_init).type(self.data_type).to(device)\n","\n","\n","        self.illumination_net = skip(self.input_depth, 3,\n","               num_channels_down = [8, 16, 32, 64, 128, 256, 512, 1024],\n","               num_channels_up   = [8, 16, 32, 64, 128, 256, 512, 1024],\n","               num_channels_skip = [0, 0, 0, 0, 0, 0, 0, 0],\n","               filter_size_down = 3, filter_size_up = 3, filter_skip_size=1,\n","               upsample_mode='bilinear',\n","               downsample_mode='avg',\n","               need_sigmoid=True, need_bias=True, pad=pad)\n","        self.illumination_net.apply(self.weight_init).type(self.data_type).to(device)\n","\n","\n","\n","    def _init_losses(self):\n","        self.l1_loss = nn.SmoothL1Loss().type(self.data_type) # for illumination\n","        self.mse_loss = nn.MSELoss().type(self.data_type)     # for reflection and reconstruction\n","        self.exclusion_loss =  ExclusionLoss().type(self.data_type)\n","        self.tv_loss = TVLoss().type(self.data_type)\n","        self.gradient_loss = GradientLoss().type(self.data_type)\n","\n","\n","\n","    def optimize(self):\n","        # torch.backends.cudnn.enabled = True\n","        # torch.backends.cudnn.benchmark = True\n","        # optimizer = SGLD(self.parameters, lr=self.learning_rate)\n","        \n","        start = time.time()\n","        for j in tqdm(range(self.num_iter)):\n","            self.optimizer.zero_grad()\n","            self.calculate_loss()\n","            self._save_result(j)\n","            self.optimizer.step()\n","            self.scheduler.step()\n","\n","        end = time.time()\n","        print(\"time:%.4f\"%(end-start))\n","        self.best_result.save(str(self.image_name))\n","        append_img(self.original_image, self.best_result, 1024, ('original', 'enhanced')).save(self.image_name.parent/('compared_'+self.image_name.name))\n","\n","    def calculate_loss(self):\n","\n","        reg_noise_std = 1e-4\n","\n","        illumination_net_input = self.illumination_net_input #+ self.illumination_net_input.clone().normal_(mean=0, std=reg_noise_std).to(device)\n","        reflection_net_input = self.reflection_net_input #+ self.reflection_net_input.clone().normal_(mean=0, std=reg_noise_std).to(device)\n","\n","\n","        self.illumination_out = self.illumination_net(illumination_net_input)\n","        self.reflection_out = self.reflection_net(reflection_net_input)\n","\n","        # weighted with the gradient of latent reflectance\n","        self.total_loss = 0.5*self.tv_loss(self.illumination_out, self.reflection_out)\n","        self.total_loss += 0.0001*self.tv_loss(self.reflection_out)\n","        # self.total_loss += 0.0001*self.tv_loss(self.illumination_out)\n","        self.total_loss += self.l1_loss(self.illumination_out, self.original_illumination/255)\n","        self.total_loss += self.mse_loss(\n","            self.illumination_out*self.reflection_out, \n","            self.image_torch/255\n","        )\n","        self.total_loss.backward()\n","\n","\n","    def _obtain_current_result(self, step):\n","        \"\"\"\n","        puts in self.current result the current result.\n","        also updates the best result\n","        :return:\n","        \"\"\"\n","        if step == self.num_iter - 1 or step % 8 == 0:\n","            reflection_out_np = np.clip(torch_to_np(self.reflection_out),0,1)\n","            illumination_out_np = np.clip(torch_to_np(self.illumination_out),0,1)\n","            # psnr = compare_psnr(np.clip(self.image_np,0,1),  reflection_out_np * illumination_out_np)\n","            # self.psnrs.append(psnr)\n","\n","            self.current_result = EnhancementResult(reflection=reflection_out_np, illumination=illumination_out_np)\n","            # if self.best_result is None or self.best_result.psnr < self.current_result.psnr:\n","            #     self.best_result = self.current_result\n","\n","    def _save_result(self, step):\n","        if (step % self.show_every == self.show_every - 1) or (step==0) or (step==self.num_iter-1):\n","            print('Iteration {:5d}    Loss {:5f}'.format(step,self.total_loss.item()))\n","\n","            self.get_enhanced(step)\n","\n","    def gamma_trans(self, img, gamma):\n","        gamma_table = [np.power(x / 255.0, gamma) * 255.0 for x in range(256)]\n","        gamma_table = np.round(np.array(gamma_table)).astype(np.uint8)\n","        return cv2.LUT(img, gamma_table)\n","\n","    def adjust_gammma(self,img):\n","        image_gamma_correct = self.gamma_trans(img, 0.5)\n","        return image_gamma_correct\n","\n","    def get_enhanced(self, step, flag=False):\n","        (R, G, B) = self.image.split()\n","        ini_illumination = self.illumination_out * 255.0\n","        ini_illumination = torch_to_np(ini_illumination).transpose(1, 2, 0) # H x W x C\n","        # If the input image is extremely dark, setting the flag as True can produce promising result.\n","        if flag==True:\n","            ini_illumination = np.clip(np.max(ini_illumination, axis=2, keepdims=True), 0.0000002, 255)\n","        else:\n","            # initial_shape = ini_illumination.shape\n","            # ini_illumination = np.max(ini_illumination, axis=2, keepdims=True)\n","            # ini_illumination = np.broadcast_to(ini_illumination, shape=initial_shape)\n","            ini_illumination = np.clip(self.adjust_gammma(ini_illumination.astype(np.uint8)), 0.0000002, 255)\n","        \n","        R = R / ini_illumination[:,:,0] # R range [0,255], ini_illu supposedly range [0, 255]\n","        G = G / ini_illumination[:,:,1]\n","        B = B / ini_illumination[:,:,2]\n","        self.best_result = Image.fromarray(np.clip(cv2.merge([R, G, B])*255, 0.000002, 255).astype(np.uint8), mode='RGB')#.resize(self.size)    \n","        self.best_result = reverse_scale_fill(self.best_result, self.size, self.scalefill_offset, self.image_resize_size)\n","        self.best_result.save('output/'+self.image_name.stem+('_{}.jpeg'.format(step)))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["66a11588308144f4acb32a9040866d12","ef434e6792d04171bc73a13ca9744172","19cd617b8dc94cbbb5136f0fe9a9bf07","c338c33197e7466cbbc6b22d55de285c","2bcc76f139aa49ebac4553f6169e96a1","5646fc4d5324405a9a94000b378f0fab","eecabb53e7d043b7b3590bf4b7b6627a","12fd479695d449c4966baf987d017a24","13b50940958147139ff99396bea2e6bd","d786b533690c4c7cb0ea8fbb8005f210","e30c291367eb453986e24f9613d20d9e"]},"executionInfo":{"elapsed":845870,"status":"ok","timestamp":1649431668696,"user":{"displayName":"Thong Phan","userId":"05546116130837665241"},"user_tz":-420},"id":"HqVNc8ofI8w4","outputId":"f1d280af-9fd6-4ced-e2a8-e3cf539f1bdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Output path =  result/5-4-2022\n","[PosixPath('/content/drive/MyDrive/fun deep learning/image-enhancement/original_images/5-4-2022/IMG_3985.HEIC'), PosixPath('/content/drive/MyDrive/fun deep learning/image-enhancement/original_images/5-4-2022/IMG_3979.HEIC'), PosixPath('/content/drive/MyDrive/fun deep learning/image-enhancement/original_images/5-4-2022/IMG_3983.HEIC'), PosixPath('/content/drive/MyDrive/fun deep learning/image-enhancement/original_images/5-4-2022/IMG_3974.HEIC'), PosixPath('/content/drive/MyDrive/fun deep learning/image-enhancement/original_images/5-4-2022/IMG_3973.HEIC'), PosixPath('/content/drive/MyDrive/fun deep learning/image-enhancement/original_images/5-4-2022/IMG_3975.HEIC'), PosixPath('/content/drive/MyDrive/fun deep learning/image-enhancement/original_images/5-4-2022/IMG_3976.HEIC')]\n","-------------Processing IMG_3985.HEIC\n","Original image size: (3024, 4032)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a11588308144f4acb32a9040866d12"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Iteration     0    Loss 0.113814\n","Iteration     9    Loss 0.052970\n","Iteration    19    Loss 0.024481\n","Iteration    29    Loss 0.015282\n","Iteration    39    Loss 0.010796\n","Iteration    49    Loss 0.008719\n","Iteration    59    Loss 0.008117\n","Iteration    69    Loss 0.007588\n","Iteration    79    Loss 0.007067\n","Iteration    89    Loss 0.006503\n","Iteration    99    Loss 0.007227\n","Iteration   109    Loss 0.006461\n","Iteration   119    Loss 0.007147\n","Iteration   129    Loss 0.006233\n","Iteration   139    Loss 0.007018\n","Iteration   149    Loss 0.005928\n","Iteration   159    Loss 0.005844\n","Iteration   169    Loss 0.005847\n","Iteration   179    Loss 0.005739\n","Iteration   189    Loss 0.005773\n","Iteration   199    Loss 0.005557\n","Iteration   209    Loss 0.005537\n","Iteration   219    Loss 0.005754\n","Iteration   229    Loss 0.005367\n","Iteration   239    Loss 0.005367\n","Iteration   249    Loss 0.005156\n","Iteration   259    Loss 0.005220\n","Iteration   269    Loss 0.005287\n","Iteration   279    Loss 0.005314\n","Iteration   289    Loss 0.005277\n","Iteration   299    Loss 0.005432\n","Iteration   309    Loss 0.004900\n","Iteration   319    Loss 0.004760\n","Iteration   329    Loss 0.004718\n","Iteration   339    Loss 0.004676\n","Iteration   349    Loss 0.004652\n","Iteration   359    Loss 0.004630\n","Iteration   369    Loss 0.004610\n","Iteration   379    Loss 0.004592\n","Iteration   389    Loss 0.004574\n","Iteration   399    Loss 0.004556\n","Iteration   409    Loss 0.004540\n","Iteration   419    Loss 0.004524\n","Iteration   429    Loss 0.004508\n","Iteration   439    Loss 0.004492\n","Iteration   449    Loss 0.004476\n","Iteration   459    Loss 0.004460\n","Iteration   469    Loss 0.004444\n","Iteration   479    Loss 0.004428\n","Iteration   489    Loss 0.004412\n","Iteration   499    Loss 0.004397\n","Iteration   509    Loss 0.004382\n","Iteration   519    Loss 0.004366\n","Iteration   529    Loss 0.004351\n","Iteration   539    Loss 0.004335\n","Iteration   549    Loss 0.004320\n","Iteration   559    Loss 0.004306\n","Iteration   569    Loss 0.004292\n","Iteration   579    Loss 0.004281\n","Iteration   589    Loss 0.004267\n","Iteration   599    Loss 0.004253\n","Iteration   609    Loss 0.004250\n","Iteration   619    Loss 0.004248\n","Iteration   629    Loss 0.004247\n","Iteration   639    Loss 0.004245\n","Iteration   649    Loss 0.004244\n","Iteration   659    Loss 0.004242\n","Iteration   669    Loss 0.004241\n","Iteration   679    Loss 0.004239\n","Iteration   689    Loss 0.004238\n","Iteration   699    Loss 0.004237\n","Iteration   709    Loss 0.004235\n","Iteration   719    Loss 0.004234\n","Iteration   729    Loss 0.004232\n","Iteration   739    Loss 0.004231\n","Iteration   749    Loss 0.004229\n","Iteration   759    Loss 0.004227\n","Iteration   769    Loss 0.004226\n","Iteration   779    Loss 0.004224\n","Iteration   789    Loss 0.004223\n","Iteration   799    Loss 0.004221\n","Iteration   809    Loss 0.004220\n","Iteration   819    Loss 0.004218\n","Iteration   829    Loss 0.004217\n","Iteration   839    Loss 0.004215\n","Iteration   849    Loss 0.004214\n","Iteration   859    Loss 0.004212\n","Iteration   869    Loss 0.004211\n","Iteration   879    Loss 0.004209\n","Iteration   889    Loss 0.004208\n","Iteration   899    Loss 0.004206\n","Iteration   909    Loss 0.004205\n","Iteration   919    Loss 0.004203\n","Iteration   929    Loss 0.004201\n","Iteration   939    Loss 0.004200\n","Iteration   949    Loss 0.004198\n","Iteration   959    Loss 0.004196\n","Iteration   969    Loss 0.004195\n","Iteration   979    Loss 0.004193\n","Iteration   989    Loss 0.004191\n","Iteration   999    Loss 0.004189\n","time:832.2816\n"]}],"source":["img_folder = Path('/content/drive/MyDrive/fun deep learning/image-enhancement/original_images/5-4-2022/')\n","if not img_folder.is_file():\n","    result_path = gu.folder(Path('./result/'+img_folder.parts[-1]))\n","else:\n","    result_path = gu.folder(Path('./result/'+img_folder.parts[-2]))\n","print('Output path = ', result_path)\n","img_path_list = iu.get_image_paths(img_folder)\n","print(img_path_list)\n","\n","for img_path in img_path_list:\n","    print('-------------Processing '+img_path.name)\n","    image = Image.open(img_path)\n","    s = Enhancement(result_path/(img_path.stem + '.jpeg'), \n","                    image, \n","                    target_size=1024,\n","                    plot_during_training=True, show_every=10, num_iter=1000)\n","    s.optimize()\n","    break\n","    \n","\n","    \n"]},{"cell_type":"code","source":[""],"metadata":{"id":"P7UOOrAHvPtq"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"main.ipynb","provenance":[]},"interpreter":{"hash":"a446cf2473893b724ba0ed2fa96f668b60f5c78cf371ac7d0f6f495aab752458"},"kernelspec":{"display_name":"Python 3.9.7 ('image')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"66a11588308144f4acb32a9040866d12":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef434e6792d04171bc73a13ca9744172","IPY_MODEL_19cd617b8dc94cbbb5136f0fe9a9bf07","IPY_MODEL_c338c33197e7466cbbc6b22d55de285c"],"layout":"IPY_MODEL_2bcc76f139aa49ebac4553f6169e96a1"}},"ef434e6792d04171bc73a13ca9744172":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5646fc4d5324405a9a94000b378f0fab","placeholder":"​","style":"IPY_MODEL_eecabb53e7d043b7b3590bf4b7b6627a","value":"100%"}},"19cd617b8dc94cbbb5136f0fe9a9bf07":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_12fd479695d449c4966baf987d017a24","max":1000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13b50940958147139ff99396bea2e6bd","value":1000}},"c338c33197e7466cbbc6b22d55de285c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d786b533690c4c7cb0ea8fbb8005f210","placeholder":"​","style":"IPY_MODEL_e30c291367eb453986e24f9613d20d9e","value":" 1000/1000 [13:52&lt;00:00,  1.02s/it]"}},"2bcc76f139aa49ebac4553f6169e96a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5646fc4d5324405a9a94000b378f0fab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eecabb53e7d043b7b3590bf4b7b6627a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12fd479695d449c4966baf987d017a24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13b50940958147139ff99396bea2e6bd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d786b533690c4c7cb0ea8fbb8005f210":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e30c291367eb453986e24f9613d20d9e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}