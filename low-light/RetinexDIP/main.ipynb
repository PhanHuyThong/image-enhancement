{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import general_utils as gu\n",
    "import img_utils as iu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from net import *\n",
    "from net.downsampler import *\n",
    "from net.losses import StdLoss, GradientLoss, ExtendedL1Loss, GrayLoss\n",
    "from net.losses import ExclusionLoss, TVLoss\n",
    "from net.noise import get_noise\n",
    "from PIL import Image\n",
    "from pillow_heif import register_heif_opener\n",
    "register_heif_opener()\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def scale_fill(im, target_width, target_height, return_offset=False):\n",
    "    '''\n",
    "    Resize PIL image keeping ratio and adding zero-valued background.\n",
    "    '''\n",
    "    target_ratio = target_height / target_width\n",
    "    im_ratio = im.height / im.width\n",
    "    if target_ratio > im_ratio:\n",
    "        # It must be fixed by width\n",
    "        resize_width = target_width\n",
    "        resize_height = round(resize_width * im_ratio)\n",
    "    else:\n",
    "        # Fixed by height\n",
    "        resize_height = target_height\n",
    "        resize_width = round(resize_height / im_ratio)\n",
    "\n",
    "    image_resize = im.resize((resize_width, resize_height), Image.ANTIALIAS)\n",
    "    background = Image.new('RGBA', (target_width, target_height), 0)\n",
    "    offset = (round((target_width - resize_width) / 2), round((target_height - resize_height) / 2))\n",
    "    background.paste(image_resize, offset)\n",
    "\n",
    "    if return_offset:\n",
    "        return background.convert('RGB'), offset, image_resize.size \n",
    "\n",
    "    return background.convert('RGB')\n",
    "\n",
    "def reverse_scale_fill(im, original_size, offset, image_resize_size):\n",
    "    '''\n",
    "    original_size: (W, H)\n",
    "    image_resize_size: (W, H) : size of the original image in the scalefill image \n",
    "    offset: (W_dim, H_dim)\n",
    "    Example: scalefill from 720, 960 to 960, 960 will have offset=120,0 and image_resize_size=720,960\n",
    "    '''\n",
    "    box = offset[0], offset[1], offset[0]+image_resize_size[0], offset[1]+image_resize_size[1]\n",
    "    return im.crop(box).resize(original_size)\n",
    "\n",
    "EnhancementResult = namedtuple(\"EnhancementResult\", ['reflection', 'illumination'])\n",
    "\n",
    "class Enhancement(object):\n",
    "    def __init__(self, image_name, image, target_size=960, plot_during_training=False, show_every=10, num_iter=300):\n",
    "        self.original_image = image\n",
    "        self.size = image.size # (height, width)\n",
    "        self.target_size = target_size\n",
    "        self.image_np = None\n",
    "        self.images_torch = None\n",
    "        self.plot_during_training = plot_during_training\n",
    "        # self.ratio = ratio\n",
    "        self.psnrs = []\n",
    "        self.show_every = show_every\n",
    "        self.image_name = image_name\n",
    "        self.num_iter = num_iter\n",
    "        self.loss_function = None\n",
    "        # self.ratio_net = None\n",
    "        self.parameters = None\n",
    "        self.learning_rate = 0.01\n",
    "        self.input_depth = 3  # This value could affect the performance. 3 is ok for natural image, if your\n",
    "                            #images are extremely dark, you may consider 8 for the value.\n",
    "        self.data_type = torch.FloatTensor\n",
    "        self.reflection_net_inputs = None\n",
    "        self.illumination_net_inputs = None\n",
    "        self.original_illumination = None\n",
    "        self.original_reflection = None\n",
    "        self.reflection_net = None\n",
    "        self.illumination_net = None\n",
    "        self.total_loss = None\n",
    "        self.reflection_out = None\n",
    "        self.illumination_out = None\n",
    "        self.current_result = None\n",
    "        self.best_result = None\n",
    "        self._init_all()\n",
    "\n",
    "        print('Original image size:', self.size)\n",
    "\n",
    "    def _init_all(self):\n",
    "        self._init_images()\n",
    "        self._init_decomposition()\n",
    "        self._init_nets()\n",
    "        self._init_inputs()\n",
    "        self._init_parameters()\n",
    "        self._init_losses()\n",
    "\n",
    "\n",
    "    def _maxRGB(self):\n",
    "        '''\n",
    "        self.image: pil image, input low-light image\n",
    "        :return: np, initial illumnation\n",
    "        '''\n",
    "        (R, G, B) = self.image.split()\n",
    "        I_0 = np.array(np.maximum(np.maximum(R, G), B))\n",
    "        return I_0\n",
    "\n",
    "    def _init_decomposition(self):\n",
    "        temp = self._maxRGB() # numpy\n",
    "        # get initial illumination map\n",
    "        #IMPORTANT: min clip value is a hyper param\n",
    "        self.original_illumination = np.clip(np.asarray([temp for _ in range(3)]), 2, 255) #range [0,255]\n",
    "        # get initial reflection\n",
    "        self.original_reflection = self.image_np / self.original_illumination #range [0, 255] / [0,255] = [0, 1]\n",
    "\n",
    "        self.original_illumination = np_to_torch(self.original_illumination).type(self.data_type)\n",
    "        self.original_reflection = np_to_torch(np.asarray(self.original_reflection)).type(self.data_type)\n",
    "\n",
    "    def _init_images(self):\n",
    "\n",
    "        self.original_image_torch = transforms.PILToTensor()(self.original_image)\n",
    "        self.image, self.scalefill_offset, self.image_resize_size = scale_fill(self.original_image, self.target_size, self.target_size, return_offset=True)\n",
    "        # self.image = transforms.Resize((512, 512))(self.original_image)\n",
    "        self.image_np = pil_to_np(self.image)  # pil image to numpy\n",
    "        self.image_torch = np_to_torch(self.image_np).type(self.data_type)\n",
    "\n",
    "    def _init_inputs(self):\n",
    "        if self.image_torch is not None:\n",
    "            size = (self.image_torch.shape[2], self.image_torch.shape[3])\n",
    "            # print(size)\n",
    "        input_type = 'noise'\n",
    "        # input_type = 'meshgrid'\n",
    "        self.reflection_net_input = get_noise(self.input_depth,\n",
    "                                                  input_type, size).type(self.data_type).detach()\n",
    "        self.illumination_net_input = get_noise(self.input_depth,\n",
    "                                             input_type, size).type(self.data_type).detach()\n",
    "\n",
    "\n",
    "    def _init_parameters(self):\n",
    "        self.parameters = [p for p in self.reflection_net.parameters()] + \\\n",
    "                          [p for p in self.illumination_net.parameters()]\n",
    "        self.optimizer = torch.optim.Adam(self.parameters, lr=self.learning_rate)\n",
    "\n",
    "\n",
    "    def weight_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0.0, 0.5 * math.sqrt(2. / n))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif classname.find('Linear') != -1:\n",
    "            n = m.weight.size(1)\n",
    "            m.weight.data.normal_(0, 0.01)\n",
    "            m.bias.data = torch.ones(m.bias.data.size())\n",
    "\n",
    "    def _init_nets(self):\n",
    "        pad = 'zero'\n",
    "        self.reflection_net = skip(self.input_depth, 3,\n",
    "               num_channels_down = [8, 16, 32, 64, 128, 256],\n",
    "               num_channels_up   = [8, 16, 32, 64, 128, 256],\n",
    "               num_channels_skip = [0, 0, 0, 0, 0, 0],\n",
    "               filter_size_down = 3, filter_size_up = 3, filter_skip_size=1,\n",
    "               upsample_mode='bilinear',\n",
    "               downsample_mode='avg',\n",
    "               need_sigmoid=True, need_bias=True, pad=pad)\n",
    "        self.reflection_net.apply(self.weight_init).type(self.data_type)\n",
    "\n",
    "\n",
    "        self.illumination_net = skip(self.input_depth, 3,\n",
    "               num_channels_down = [8, 16, 32, 64, 128, 256],\n",
    "               num_channels_up   = [8, 16, 32, 64, 128, 256],\n",
    "               num_channels_skip = [0, 0, 0, 0, 0, 0],\n",
    "               filter_size_down = 3, filter_size_up = 3, filter_skip_size=1,\n",
    "               upsample_mode='bilinear',\n",
    "               downsample_mode='avg',\n",
    "               need_sigmoid=True, need_bias=True, pad=pad)\n",
    "        self.illumination_net.apply(self.weight_init).type(self.data_type)\n",
    "\n",
    "\n",
    "\n",
    "    def _init_losses(self):\n",
    "        self.l1_loss = nn.SmoothL1Loss().type(self.data_type) # for illumination\n",
    "        self.mse_loss = nn.MSELoss().type(self.data_type)     # for reflection and reconstruction\n",
    "        self.exclusion_loss =  ExclusionLoss().type(self.data_type)\n",
    "        self.tv_loss = TVLoss().type(self.data_type)\n",
    "        self.gradient_loss = GradientLoss().type(self.data_type)\n",
    "\n",
    "\n",
    "\n",
    "    def optimize(self):\n",
    "        # torch.backends.cudnn.enabled = True\n",
    "        # torch.backends.cudnn.benchmark = True\n",
    "        # optimizer = SGLD(self.parameters, lr=self.learning_rate)\n",
    "        \n",
    "        start = time.time()\n",
    "        for j in tqdm(range(self.num_iter)):\n",
    "            self.optimizer.zero_grad()\n",
    "            self.calculate_loss()\n",
    "            if self.plot_during_training:\n",
    "                self._save_result(j)\n",
    "            self.optimizer.step()\n",
    "        end = time.time()\n",
    "        print(\"time:%.4f\"%(end-start))\n",
    "        self.best_result.save(str(self.image_name))\n",
    "\n",
    "    def calculate_loss(self):\n",
    "\n",
    "        # reg_noise_std = 1 / 10000.\n",
    "\n",
    "        illumination_net_input = self.illumination_net_input #+ \\\n",
    "                                #  self.illumination_net_input.clone().normal_(mean=0, std=reg_noise_std)\n",
    "        reflection_net_input = self.reflection_net_input #+ \\\n",
    "                            #    self.reflection_net_input.clone().normal_(mean=0, std=reg_noise_std)\n",
    "\n",
    "\n",
    "        self.illumination_out = self.illumination_net(illumination_net_input)\n",
    "        self.reflection_out = self.reflection_net(reflection_net_input)\n",
    "\n",
    "        # weighted with the gradient of latent reflectance\n",
    "        self.total_loss = 0.5*self.tv_loss(self.illumination_out, self.reflection_out)\n",
    "        self.total_loss += 0.0001*self.tv_loss(self.reflection_out)\n",
    "        self.total_loss += self.l1_loss(self.illumination_out, self.original_illumination/255)\n",
    "        self.total_loss += self.mse_loss(\n",
    "            self.illumination_out*self.reflection_out, \n",
    "            self.image_torch/255\n",
    "        )\n",
    "        self.total_loss.backward()\n",
    "\n",
    "\n",
    "    def _obtain_current_result(self, step):\n",
    "        \"\"\"\n",
    "        puts in self.current result the current result.\n",
    "        also updates the best result\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if step == self.num_iter - 1 or step % 8 == 0:\n",
    "            reflection_out_np = np.clip(torch_to_np(self.reflection_out),0,1)\n",
    "            illumination_out_np = np.clip(torch_to_np(self.illumination_out),0,1)\n",
    "            # psnr = compare_psnr(np.clip(self.image_np,0,1),  reflection_out_np * illumination_out_np)\n",
    "            # self.psnrs.append(psnr)\n",
    "\n",
    "            self.current_result = EnhancementResult(reflection=reflection_out_np, illumination=illumination_out_np)\n",
    "            # if self.best_result is None or self.best_result.psnr < self.current_result.psnr:\n",
    "            #     self.best_result = self.current_result\n",
    "\n",
    "    def _save_result(self, step):\n",
    "        if (step % self.show_every == self.show_every - 1) or (step==0) or (step==self.num_iter-1):\n",
    "            print('Iteration {:5d}    Loss {:5f}'.format(step,self.total_loss.item()))\n",
    "\n",
    "            self.get_enhanced(step)\n",
    "\n",
    "    def gamma_trans(self, img, gamma):\n",
    "        gamma_table = [np.power(x / 255.0, gamma) * 255.0 for x in range(256)]\n",
    "        gamma_table = np.round(np.array(gamma_table)).astype(np.uint8)\n",
    "        return cv2.LUT(img, gamma_table)\n",
    "\n",
    "    def adjust_gammma(self,img):\n",
    "        image_gamma_correct = self.gamma_trans(img, 0.5)\n",
    "        return image_gamma_correct\n",
    "\n",
    "    def get_enhanced(self, step, flag=False):\n",
    "        (R, G, B) = self.image.split()\n",
    "        ini_illumination = self.illumination_out * 255.0\n",
    "        ini_illumination = torch_to_np(ini_illumination).transpose(1, 2, 0) # H x W x C\n",
    "\n",
    "        # If the input image is extremely dark, setting the flag as True can produce promising result.\n",
    "        if flag==True:\n",
    "            ini_illumination = np.clip(np.max(ini_illumination, axis=2), 0.0000002, 255)\n",
    "        else:\n",
    "            ini_illumination = np.clip(self.adjust_gammma(ini_illumination.astype(np.uint8)), 0.0000002, 255)\n",
    "        \n",
    "        R = R / ini_illumination[:,:,0] # R range [0,255], ini_illu supposedly range [0, 255]\n",
    "        G = G / ini_illumination[:,:,1]\n",
    "        B = B / ini_illumination[:,:,2]\n",
    "        self.best_result = Image.fromarray(np.clip(cv2.merge([R, G, B])*255, 0.02, 255).astype(np.uint8), mode='RGB')#.resize(self.size)    \n",
    "        self.best_result = reverse_scale_fill(self.best_result, self.size, self.scalefill_offset, self.image_resize_size)\n",
    "        self.best_result.save('output/'+self.image_name.stem+('_{}.png'.format(step)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img_folder = Path('/Users/thong/Desktop/image-enhancement/original_images/5-4-2022')\n",
    "output_path = gu.folder(Path('./result/'+img_folder.parts[-1]))\n",
    "print('Output path = ', output_path)\n",
    "img_path_list = iu.get_image_paths(img_folder)\n",
    "print(img_path_list)\n",
    "\n",
    "for img_path in img_path_list:\n",
    "    print('-------------Processing '+img_path.name)\n",
    "    image = Image.open(img_path)\n",
    "    s = Enhancement(output_path/img_path.name.replace('original', 'best'), \n",
    "                    image, \n",
    "                    target_size=960,\n",
    "                    plot_during_training=True, show_every=50, num_iter=200)\n",
    "    s.optimize()\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Past stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--input\", \"-i\", type=str, default='data/Test', help='test image folder')\n",
    "# parser.add_argument(\"--result\", \"-r\", type=str, default='./result', help='result folder')\n",
    "# arg = parser.parse_args()\n",
    "\n",
    "\n",
    "img_path = Path('/Users/thong/Desktop/image-enhancement/original_20-3-2022_1.jpeg')\n",
    "output_path = Path('./result')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "img = Image.open(img_path).convert('RGB') #LOLdataset/eval15/low/1.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.transpose(Image.ROTATE_270)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (3456, 4608)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s = Enhancement(output_path/img_path.name.replace('original', 'best'), img, plot_during_training=True, show_every=10, num_iter=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a446cf2473893b724ba0ed2fa96f668b60f5c78cf371ac7d0f6f495aab752458"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('image')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
